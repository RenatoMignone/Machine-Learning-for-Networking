{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ea060a47a6211d",
   "metadata": {},
   "source": [
    "# LAB #2: Numpy\n",
    "\n",
    "## Introduction\n",
    "In this laboratory, you will perform some operation with NumPy arrays in such a way to build your first Machine Learning model. \n",
    "In particular, you will build a NumPy-based version of the K-Nearest Neighbors algorithm (a.k.a. KNN).\n",
    "\n",
    "## 0 Preliminary steps\n",
    "### 0.1 NumPy\n",
    "Make sure you have the NumPy library installed, its use is strongly recommended for this laboratory.\n",
    "NumPy is the fundamental package for scientific computing with Python. You can read more about it on\n",
    "the official documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9246699975edf562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:11.544310Z",
     "start_time": "2024-10-10T16:08:09.387743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\gabri\\.conda\\envs\\mu_cem\\lib\\site-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad497ed1d0092203",
   "metadata": {},
   "source": [
    "### 0.2 Iris dataset download \n",
    "For this lab, you will need two of the datasets you have already met: Iris and MNIST. Please refer to\n",
    "Laboratory 1 for a complete description of the datasets.\n",
    "Iris. You can download it from:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a838a5ed77a24051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.158423Z",
     "start_time": "2024-10-10T16:08:11.545409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\gabri\\.conda\\envs\\mu_cem\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iris (2).csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linux users\n",
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -O iris.csv\n",
    "# windows users\n",
    "! pip install wget\n",
    "import wget\n",
    "wget.download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \"iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef169d9060adb9a7",
   "metadata": {},
   "source": [
    "## 1 Exercises \n",
    "Note that exercises marked with a ($\\star$) are optional, you should focus on completing the other ones first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820274dc6b6f678",
   "metadata": {},
   "source": [
    "## 1.1 Iris Analysis with Numpy\n",
    "As you might remember from Lab. 1, the Iris dataset collects the measurements of different Iris flowers,\n",
    "and each data point is characterized by 4 **features** (sepal length, sepal width, petal length, petal width) and is associated to 1 **label** (i.e. an Iris species - Setosa, Versicolor, or Virginica) which in this case is the last element of the row (last column of the csv file). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46864c46cf9f9387",
   "metadata": {},
   "source": [
    "1. Load the Iris dataset. You can use the `csv` library that we saw in the last laboratory or read it with the standard `open(filename, strategy)`. \n",
    "In the second case remember to split correctly the different fields, and avoid new line characters. In any case check for empty lines. \n",
    "This time remember to store the 4 features in a numpy array `x` of shape (n_sample, 4) and the labels in a different array `y` of shape (n_sample,) converting the 3 different species to a corresponding numerical value. E.g.,\n",
    "      - Iris-setosa: 0\n",
    "      - Iris-versicolor: 1\n",
    "      - Iris-virginica: 2\n",
    "\n",
    "In order to check you have correctly loaded the data, print the shape of the two arrays: you should find\n",
    "(150, 4) for `x` and (150,) for `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a977ccc88ef2ca39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.252553Z",
     "start_time": "2024-10-10T16:08:14.159561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (150, 4)\n",
      "y.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "labels = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2\n",
    "}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "with open(\"iris.csv\") as f:\n",
    "    for i, cols in enumerate(csv.reader(f)):\n",
    "        if cols != []:\n",
    "            x.append(cols[:4])\n",
    "            label_num = labels[cols[4]]\n",
    "            y.append(label_num)\n",
    "            \n",
    "x = np.array(x, dtype=np.float32)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"y.shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050d162966956ce",
   "metadata": {},
   "source": [
    "2. Compute again the mean and standard deviation for each class by means of the numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33bfaed602d4bc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.257848Z",
     "start_time": "2024-10-10T16:08:14.253617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Means: [5.0059996  3.4180002  1.464      0.24399997]\n",
      "Stds: [0.34894693 0.37719488 0.17176732 0.10613199]\n",
      "\n",
      "Class: 1\n",
      "Means: [5.936002  2.77      4.26      1.3259999]\n",
      "Stds: [0.5109834  0.3106445  0.46518815 0.19576517]\n",
      "\n",
      "Class: 2\n",
      "Means: [6.5879993 2.9740002 5.5520005 2.0260003]\n",
      "Stds: [0.6294887  0.31925544 0.5463478  0.27188972]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class_means_std = {}\n",
    "for label in np.unique(y):\n",
    "    x_class = x[y == label] # selecting the rows with the current label\n",
    "    means = x_class.mean(axis=0)\n",
    "    stds = x_class.std(axis=0)\n",
    "    class_means_std[label] = {\"mean\": means,\n",
    "                              \"std\": stds}\n",
    "    \n",
    "    print(\"Class:\", label)\n",
    "    print(\"Means:\", means)\n",
    "    print(\"Stds:\", stds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84beb708797ba9",
   "metadata": {},
   "source": [
    "3. Compute the distances among two samples (e.g., the $36^{th}$ and the $81^{th}$, the $13^{th}$ and the $15^{th}$) \n",
    "by means of the `np.linalg.norm(a-b)` function which computes the norm of `a-b`, i.e., the euclidean distance between the feature of the `a` and of the `b` samples. \n",
    "  - Can you guess if the two couples of samples belong to the same species?\n",
    "  - From the mean and standard deviations computed before can you guess which species? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a47fb722be07fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.264466Z",
     "start_time": "2024-10-10T16:08:14.259993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 36th and 81th sample: 2.9086077\n",
      "Distance between 13th and 15th sample: 1.4317821\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "print(\"Distance between 36th and 81th sample:\", euclidean_distance(x[35], x[80]))\n",
    "print(\"Distance between 13th and 15th sample:\", euclidean_distance(x[12], x[14]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc024bce0c0dd04",
   "metadata": {},
   "source": [
    "It is more likely that the 12th and 14th samples belong to the same species, since they are closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd802b47b8519bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.269761Z",
     "start_time": "2024-10-10T16:08:14.265558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35th sample: [5.  3.2 1.2 0.2]\n",
      "80th sample: [5.5 2.4 3.8 1.1]\n",
      "12th sample: [4.8 3.  1.4 0.1]\n",
      "14th sample: [5.8 4.  1.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"35th sample:\", x[35])\n",
    "print(\"80th sample:\", x[80])\n",
    "print(\"12th sample:\", x[12])\n",
    "print(\"14th sample:\", x[14])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa448bd7bc9d94",
   "metadata": {},
   "source": [
    "The 12th, 14th and 35th sample are likely to belong to the first class, since their values are closer to the mean of the first class.\n",
    "The 80th sample is likely to belong to the second class, since its values are closer to the mean of the second class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcceaccd4a1a7526",
   "metadata": {},
   "source": [
    "4. Find the k nearest neighbors of a sample in the dataset.\n",
    "    - Define a function `k_nearest_neighbors(x, x_set, k)` that takes as input a sample `x` and a set of sample (i.e., a matrix) `x_set` and returns the indices of the `k` nearest neighbors of `x` in `x_set`.\n",
    "        - Reuse the `euclidean_distance` function that you defined before to do so. \n",
    "        - Remember that the `x_set` is a matrix of shape ($N_{samples}, N_{features}$), so you have to compute the distance between `x` and each row of `x_set`. \n",
    "        - In order to find the indices of the `k` nearest neighbors, you can use the `argsort` function that returns the indices that would sort an array\n",
    "    - Apply the function to the $36^{th}$ sample of the dataset with $k=5$.\n",
    "    - Print the indices of the $5$ nearest neighbors.\n",
    "    - Print the labels of the $5$ nearest neighbors. Can you guess the label of the $36^{th}$ sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93f94748b3841e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:08:14.275037Z",
     "start_time": "2024-10-10T16:08:14.270297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the 5 nearest neighbors: [35 49  1  2 40]\n",
      "Labels of the 5 nearest neighbors: [0 0 0 0 0]\n",
      "Label of the 36th sample: 0\n"
     ]
    }
   ],
   "source": [
    "def k_nearest_neighbors(x, x_set, k):\n",
    "    # compute the distances of x from each point in x_set\n",
    "    distances = np.linalg.norm(x - x_set, axis=1)\n",
    "    # we then take the first k elements along the second axis (the training sample one) of the sorted array with [:k]\n",
    "    indices = distances.argsort()[:k]\n",
    "    return indices\n",
    "\n",
    "n_neigbours = 5\n",
    "sample_idx = 35\n",
    "\n",
    "neighbours_idx = k_nearest_neighbors(x[sample_idx], x, n_neigbours)\n",
    "print(\"Indices of the 5 nearest neighbors:\", neighbours_idx)\n",
    "print(\"Labels of the 5 nearest neighbors:\", y[neighbours_idx])\n",
    "\n",
    "print(\"Label of the 36th sample:\", y[sample_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2b1c8798fc98e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd1f94b256663e8",
   "metadata": {},
   "source": [
    "## 1.2 KNN design and implementation\n",
    "In this exercise, you will implement your own version of the K-Nearest Neighbors (KNN) algorithm, and you will use it to assign an\n",
    "Iris species (i.e. a label) to flowers whose species is unknown.\n",
    "\n",
    "The KNN algorithm is straightforward. Suppose that some measurements (e.g., the iris features) and their\n",
    "relative label (e.g., the iris species) of a set of samples are known in advance. \n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img2.png\" width=\"800\">\n",
    "\n",
    "Then, whenever we want to label a new sample, we look at the K most similar points (a.k.a. neighbors) and assign a label accordingly. \n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img1-1.png\" width=\"800\">\n",
    "\n",
    "\n",
    "The simplest solution is using a majority voting scheme: if the majority of the neighbors votes for a label, we will go for it. \n",
    "This approach is naive only at first sight: the local similarity assumed by KNN happens to be roughly true, as you have seen in the previous exercises.\n",
    "Even though this reasoning does not generalize well, the KNN provides a valid baseline for your tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d185976071690ce",
   "metadata": {},
   "source": [
    "1. Let’s identify a portion of our data for which we will try to guess the species. Randomly select 20%\n",
    "of the records and store the first four columns (i.e. the features representing each flower) into a\n",
    "two-dimensional numpy array of shape ($N_{test} \\times 4$), you can call it `X_test` and $N_{test}$ is the 20% of the total number of samples.\n",
    "For the same records, store the test label column (i.e. the one with the species values) into another array, namely `y_test`. \n",
    "This is the data that will be used to test the accuracy of your KNN implementation and its correct functioning (i.e. the testing data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a642f03b563650e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:10:46.552776Z",
     "start_time": "2024-10-10T16:10:46.549077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "indices = np.arange(150)\n",
    "np.random.shuffle(indices)\n",
    "test_len = 20*len(indices) // 100\n",
    "test_idx = indices[:test_len]\n",
    "\n",
    "x_test = x[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(\"Number of test samples:\", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5663358e8e82",
   "metadata": {},
   "source": [
    "2. Store the remaining 80% of the records in the same way. In this case, use the names X_train andy_train for the arrays.\n",
    "This is the data that your model will use as ground-truth knowledge (i.e. the training data, from which we extract the knowledge and that we will use for comparison).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f1639cc7fe3b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:10:48.395826Z",
     "start_time": "2024-10-10T16:10:48.390640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 120\n"
     ]
    }
   ],
   "source": [
    "train_idx = indices[test_len:]\n",
    "\n",
    "x_train = x[train_idx]\n",
    "y_train = y[train_idx]\n",
    "print(\"Number of training samples:\", y_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc62af2fef1d5c",
   "metadata": {},
   "source": [
    "3. Focus now on the KNN technique. \n",
    "From the next month, you will use the `scikit-learn` package. Many of its functionalities\n",
    "are exposed via an object-oriented interface. With this paradigm in mind, implement now the KNN\n",
    "algorithm and expose it as a Python class. The bare skeleton of your class should look like this (you\n",
    "are free to add other methods if you want to).\n",
    "\n",
    "```\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        Store the value of k in a attribute of the class and initialize other attributes.\n",
    "        :param k : int, number of neighbors to consider.\n",
    "        \"\"\"\n",
    "        pass # TODO: implement it!\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store the 'prior knowledge' of you model that will be used\n",
    "        to predict new labels.\n",
    "        :param X : input data points, ndarray, shape = (R,C).\n",
    "        :param y : input labels, ndarray, shape = (R,).\n",
    "        \"\"\"\n",
    "        pass # TODO: implement it!\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Run the KNN classification on X.\n",
    "        :param X: input data points, ndarray, shape = (N,C).\n",
    "        :return: labels : ndarray, shape = (N,).\n",
    "        \"\"\"\n",
    "        pass # TODO: implement it!\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Implement the `__init__` and `fit` methods first. \n",
    "- In the `__init__` method, you should store the value of `k` in a private attribute of the class.\n",
    "- In the `fit` method you should only store the training data in private attributes of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5de6a78df7f8585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:10:50.208423Z",
     "start_time": "2024-10-10T16:10:50.203073Z"
    }
   },
   "outputs": [],
   "source": [
    "class KNearestNeighbors:\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        Store the value of k in a attribute of the class and initialize other attributes.\n",
    "        :param k : int, number of neighbors to consider.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.__X_train = None\n",
    "        self.__y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store the 'prior knowledge' of you model that will be used\n",
    "        to predict new labels.\n",
    "        :param X : input data points, ndarray, shape = (R,C).\n",
    "        :param y : input labels, ndarray, shape = (R,).\n",
    "        \"\"\"\n",
    "        self.__X_train = X\n",
    "        self.__y_train = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73a4620b9c738f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad6f4fc7071bff0",
   "metadata": {},
   "source": [
    "4. Implement the `predict` method. The function receives as input a numpy array with N rows and C\n",
    "columns, corresponding to N flowers. The method assigns to each row one of the three Iris species \n",
    "using the KNN algorithm, and returns the predicted species as a numpy array. \n",
    "\n",
    "    - For the actual implementation, you can either re-use the previously defined `k_nearest_neighbors` function or \n",
    "implement a new one exploiting the numpy broadcasting capabilities in order to avoid iterating over the sample matrix `X`.\n",
    "    - Then, assign the *predicted label* to each sample using a majority voting scheme, i.e., the label that appears most frequently among the k nearest neighbors. To do so you can use the `np.unique(neighbours_labels, return_count=True)` function that returns the unique labels and their counts. \n",
    "    - Finally, return the predicted labels as a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c227627e47cc7253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:48:21.690489Z",
     "start_time": "2024-10-10T16:48:21.684487Z"
    }
   },
   "outputs": [],
   "source": [
    "class KNearestNeighbors(KNearestNeighbors): # this is a trick to extend an already existing class in Python, you simply define a child class with the same name of the parent class \n",
    "\n",
    "    def __init__(self, k):\n",
    "        super().__init__(k)\n",
    "\n",
    "    # we create a __distance method, because later way may want to extend the functionalities of the model\n",
    "    # and we will only reimplement this method\n",
    "    def __distance(self, a, b):\n",
    "        return np.linalg.norm(a-b, axis=-1)\n",
    "    \n",
    "    def __nearest_neighbors(self, x):\n",
    "        distances = self.__distance(x, self.__X_train)\n",
    "        indices = distances.argsort()[:self.k]\n",
    "        return indices\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Run the KNN classification on X.\n",
    "        :param X: input data points, ndarray, shape = (N,C).\n",
    "        :return: labels : ndarray, shape = (N,).\n",
    "        \"\"\"\n",
    "        \n",
    "        # first check that the model has been trained\n",
    "        if self.__X_train is None or self.__y_train is None:\n",
    "            raise Exception(\"The model has not been trained yet!\")\n",
    "        \n",
    "        # compute distance of X from each point in X_train\n",
    "        # the distance vector has to be of shape (R, N) \n",
    "        # where R is the number of samples of the test set (30 in IRIS) and N is the number of sample of the train set (120 in IRIS)\n",
    "        # because we want to compute the distance of each sample in the test set from each sample in the train set\n",
    "\n",
    "        # solution 1\n",
    "        # we reuse the previously defined k_nearest_neighbors function to compute the distance element-wise\n",
    "        # for each sample in the test set\n",
    "        indices = []\n",
    "        for i in range(X.shape[0]):\n",
    "           indices.append(self.__nearest_neighbors(X[i]))\n",
    "        indices = np.array(indices)\n",
    "        \n",
    "        # # solution 2\n",
    "        # # we reshape the train set to have shape (N, 1, C) so that we can broadcast it with the test set of shape (R, C) and compute the distance\n",
    "        # # here C is the number of features (4 in the Iris dataset)\n",
    "        # X = X.reshape(X.shape[0], 1, x_test.shape[1])        \n",
    "        # # we then compute the distance along the axis that we want to collapse, i.e., the axis 2 which corresponds to the features\n",
    "        # distances = self.__distance(X, self.__X_train)\n",
    "        # # then we sort the distances and take the first k elements as previously done\n",
    "        # indices = distances.argsort(axis=1)[:, :self.k]\n",
    "        \n",
    "        \n",
    "        # we then take the labels of the k nearest neighbors for each sample in the test set\n",
    "        # we use the indices to select the labels of the k nearest neighbors\n",
    "        # the final matrix has shape (R, k) where R is the number of samples in the test set and k is the number of neighbors\n",
    "\n",
    "        # solution 1 we iterate over the indices and select the labels\n",
    "        neighbour_labels = []\n",
    "        for i in range(indices.shape[0]):\n",
    "           neighbour_labels.append(self.__y_train[indices[i]])\n",
    "        neighbour_labels = np.vstack(neighbour_labels)\n",
    "        \n",
    "        # # solution 2 we simply use the indices to select the labels. This is an advanced features of fancy indexing in numpy \n",
    "        # # it returns an array of the same shape as the indices array\n",
    "        # neighbour_labels = self.__y_train[indices]\n",
    "    \n",
    "        # we then compute the majority voting\n",
    "        # we use the unique function to get the unique labels and the counts of each label\n",
    "        # we then sort the counts in descending order and take the first element\n",
    "        # the first element is the label that appears the most in the array\n",
    "        predictions = []\n",
    "        for i in range(neighbour_labels.shape[0]):\n",
    "           unique_labels, label_counts = np.unique(neighbour_labels[i], return_counts=True)\n",
    "           sorted_counts = label_counts.argsort()[::-1][0] # we sort the counts in descending order and select the most frequent label\n",
    "           predictions.append(unique_labels[sorted_counts])\n",
    "        predictions = np.array(predictions)    \n",
    "        \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd1131d3ba785d",
   "metadata": {},
   "source": [
    "5. Now let’s fit the KNN model with the X_train and y_train data. Then, try to use your KNN model\n",
    "to predict the species for each record in X_test and store them in a nupy array called y_pred.\n",
    "As we did in the previous lab, check how many Iris species in the array y_pred have been guessed correctly computing with respect to the ones in y_test computing the accuracy. \n",
    "    - A prediction is correct if `y_pred[i] == y_test[i]`. To get the accuracy then compute the ratio between the number of correct guesses and the total number of guesses is known. \n",
    "    - If all labels are assigned correctly ((y_pred == y_test).all() == True), the accuracy of the model is 100%. \n",
    "    - Instead, if none of the guessed species corresponds to the real one ((y_pred == y_test).any() == False), the accuracy is 0%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca4f0b4bbe44c9fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:48:24.441032Z",
     "start_time": "2024-10-10T16:48:24.436095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 2 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n",
      "True labels: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n",
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNearestNeighbors(k=3)\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict(x_test)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:\", y_test)\n",
    "\n",
    "accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514fc82de74b729",
   "metadata": {},
   "source": [
    "6. ($\\star$) As a software developer, you might want to increase the functionalities of your product and\n",
    "publish newer versions over time. The better your code is structured and organized, the lower is the\n",
    "effort to release updates.\n",
    "As such,  extend your KNN implementation adding the parameter `distance`. This has to be one among:\n",
    "    - Euclidean distance: $ euclidean(p,q) = \\sqrt{\\sum_{i=1}^{n} (p_i _- q_i)^2} $\n",
    "    - Manhattan distance: $ manhattan(p,q) = \\sum_{i=1}^n |p_i - q_i|$\n",
    "    - Cosine distance: $ cosine(p, q) = 1 - \\frac{\\sum_{i=1}^n p_i q_i}{ \\sqrt{\\sum^n_{i=1} p^2_i} \\cdot \\sqrt{\\sum^n_{i=1} q_i^2}}$\n",
    "\n",
    "If any of this distance is not already implemented in `numpy` implement it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "436c6395a2f3d853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:06:48.388093Z",
     "start_time": "2024-10-10T13:06:48.377378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [2 2 2 1 1 1 0 2 0 0 0 0 1 1 1 2 1 0 0 1 2 0 2 0 2 0 1 0 0 2]\n",
      "True labels: [2 2 2 1 1 1 0 2 0 0 0 0 1 1 1 2 1 0 0 1 2 0 2 0 2 0 1 0 0 2]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# let's extend the KNearestNeighbors class to also accept the distance metric\n",
    "class KNearestNeighbors(KNearestNeighbors):\n",
    "    def __init__(self, k, distance_metric=\"euclidean\"):\n",
    "        super().__init__(k)\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "    def __distance(self, a, b):\n",
    "        if self.distance_metric == \"euclidean\":\n",
    "            distances = np.linalg.norm(a-b, axis=-1)\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            distances = np.linalg.norm(a-b, ord=1, axis=-1)\n",
    "        elif self.distance_metric == \"cosine\":\n",
    "            distances = 1 - np.sum(a * b, axis=-1) / (np.linalg.norm(a, axis=-1) * np.linalg.norm(b, axis=-1))\n",
    "        else:\n",
    "            raise Exception(\"Unknown distance metric!\")\n",
    "        \n",
    "        return distances\n",
    "\n",
    "knn_model = KNearestNeighbors(k=3, distance_metric=\"cosine\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict(x_test)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:\", y_test)\n",
    "\n",
    "accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c76d735fe65dbd",
   "metadata": {},
   "source": [
    "\n",
    "7. ($\\star$) Again, extend now your KNN implementation by adding the parameter `weights` to the constructor,\n",
    "as shown below:\n",
    "\n",
    "```\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k, distance_metric=\"euclidean\", weights=\"uniform\"):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weights = weights\n",
    "```\n",
    "\n",
    "Change your KNN implementation to accept a new weighting scheme for the labels. If weights=\n",
    "\"distance\", weight neighbor votes by the inverse of their distance (for the distance, again, use\n",
    "distance_metric). The weight for a neighbor of the point p is:\n",
    "\n",
    "$\n",
    "w(p, n) = \\frac{1}{distance\\_metric(p, n)}\n",
    "$\n",
    "\n",
    "Instead, if the default is chosen (weights=\"uniform\"), use the majority voting you already implemented\n",
    "in Exercise 6.\n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img5.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84262b9fd13d9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:07:33.497132Z",
     "start_time": "2024-10-10T13:07:33.483948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [2 2 2 1 1 1 0 2 0 0 0 0 1 1 1 2 1 0 0 1 2 0 2 0 2 0 1 0 0 2]\n",
      "True labels: [2 2 2 1 1 1 0 2 0 0 0 0 1 1 1 2 1 0 0 1 2 0 2 0 2 0 1 0 0 2]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "class KNearestNeighbors(KNearestNeighbors):\n",
    "    def __init__(self, k, distance_metric=\"euclidean\", weights=\"uniform\"):\n",
    "        super().__init__(k, distance_metric)\n",
    "        self.weights = weights\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Run the KNN classification on X.\n",
    "        :param X: input data points, ndarray, shape = (N,C).\n",
    "        :return: labels : ndarray, shape = (N,).\n",
    "        \"\"\"\n",
    "        \n",
    "        # first check that the model has been trained\n",
    "        if self.__X_train is None or self.__y_train is None:\n",
    "            raise Exception(\"The model has not been trained yet!\")\n",
    "        \n",
    "        # compute distance of X from each point in X_train\n",
    "        X = X.reshape(X.shape[0], 1, x_test.shape[1])        \n",
    "\n",
    "        # we then compute the distance along the axis that we want to collapse, i.e., the axis 2 which corresponds to the features\n",
    "        distances = self.__distance(X, self.__X_train)\n",
    "        \n",
    "        # then we sort the distances and take the first k elements\n",
    "        indices = distances.argsort(axis=1)[:, :self.k]\n",
    "        \n",
    "        # we then take the labels of the k nearest neighbors for each sample in the test set\n",
    "        neighbour_labels = self.__y_train[indices]\n",
    "    \n",
    "        # we then compute the weighted majority voting by considering\n",
    "        predictions = []\n",
    "        \n",
    "        if self.weights == \"uniform\":\n",
    "            for i in range(neighbour_labels.shape[0]):\n",
    "                unique_labels, label_counts = np.unique(neighbour_labels[i], return_counts=True)\n",
    "                sorted_counts = label_counts.argsort()[::-1][0] # we sort the counts in descending order and select the most frequent label\n",
    "                predictions.append(unique_labels[sorted_counts])\n",
    "            \n",
    "        elif self.weights == \"distance\":\n",
    "            for i in range(neighbour_labels.shape[0]):\n",
    "                label_count = {}\n",
    "                for j in range(neighbour_labels.shape[1]):\n",
    "                    label = neighbour_labels[i, j]\n",
    "                    index = indices[i, j]\n",
    "                    if label not in label_count:\n",
    "                        label_count[label] = 1 / distances[i, index]\n",
    "                    else:\n",
    "                        label_count[label] += 1 / distances[i, index]\n",
    "                predictions.append(max(label_count, key=label_count.get))\n",
    "        else:\n",
    "            raise Exception(\"Unknown weights!\")\n",
    "        predictions = np.array(predictions)    \n",
    "        \n",
    "        \n",
    "        return predictions \n",
    "\n",
    "\n",
    "knn_model = KNearestNeighbors(k=5, distance_metric=\"cosine\", weights=\"distance\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict(x_test)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:\", y_test)\n",
    "accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1e2a662695741",
   "metadata": {},
   "source": [
    "8. ($\\star$) Test the modularity of the implementation applying it on a different dataset. Ideally, you should\n",
    "not change the code of your KNN python class.\n",
    "- Download the MNIST dataset and retain only 100 samples per digit. You will end up with a dataset of 1000 samples.\n",
    "- Define again four numpy arrays as you did in Exercises 2 and 3.\n",
    "- Apply your KNN as you did for the Iris dataset.\n",
    "- Evaluate the accuracy on MNIST’s y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b720ef714195eb68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:08:12.156900Z",
     "start_time": "2024-10-10T13:08:09.569799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist (1).csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download MNIST dataset\n",
    "! pip install wget\n",
    "\n",
    "import wget\n",
    "wget.download(\"https://raw.githubusercontent.com/dbdmg/data-science-lab/master/datasets/mnist_test.csv\", \"mnist.csv\")\n",
    "#! wget https://raw.githubusercontent.com/dbdmg/data-science-lab/master/datasets/mnist_test.csv -O mnist.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77afcee410ef94ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:08:36.600059Z",
     "start_time": "2024-10-10T13:08:35.573140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000,)\n",
      "{7: 100, 2: 100, 1: 100, 0: 100, 4: 100, 9: 100, 5: 100, 6: 100, 3: 100, 8: 100}\n"
     ]
    }
   ],
   "source": [
    "# extracting MNIST dataset\n",
    "samples_per_digit = {}\n",
    "x, y = [], []\n",
    "with open(\"mnist.csv\", \"r\") as f:\n",
    "    for columns in csv.reader(f):\n",
    "\n",
    "        # check empty lines\n",
    "        if len(columns) == 0:\n",
    "            break\n",
    "\n",
    "        # first element in MNIST is the label (i.e., value of the corresponding written digit), the other are the gray-scaled pixel intensity \n",
    "        features = np.array(columns[1:], dtype=np.float32)\n",
    "        label = int(columns[0])\n",
    "            \n",
    "        # sample only 100 points per digit\n",
    "        if label in samples_per_digit:\n",
    "            if samples_per_digit[label] >= 100:\n",
    "                continue\n",
    "            else:\n",
    "                samples_per_digit[label] += 1\n",
    "        else:\n",
    "            samples_per_digit[label] = 1\n",
    "        \n",
    "        x.append(features)\n",
    "        y.append(label)        \n",
    "        \n",
    "x = np.vstack(x) # vstack to extract a new dimension and have a 2D array (number of samples, number of features)\n",
    "y = np.array(y) # y is simply a list of integer, so we create a 1D array (number of samples,)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "    \n",
    "print(samples_per_digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a0834dd8885a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:08:38.144695Z",
     "start_time": "2024-10-10T13:08:38.134111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples (800,)\n",
      "Number of test samples (200,)\n"
     ]
    }
   ],
   "source": [
    "# define four numpy arrays x_train, y_train, x_test, y_test\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "test_len = 20*len(indices) // 100\n",
    "test_idx = indices[:test_len]\n",
    "train_idx = indices[test_len:]\n",
    "\n",
    "x_test = x[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "x_train = x[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "print(\"Number of training samples\", y_train.shape)\n",
    "print(\"Number of test samples\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c03d2add840c1531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T13:09:43.386963Z",
     "start_time": "2024-10-10T13:09:43.179378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [4 3 3 3 3 3 0 7 1 0 1 1 2 5 8 5 6 9 5 5 0 8 0 1 6 4 9 9 3 8 8 4 9 2 1 9 3\n",
      " 0 8 0 3 1 4 8 6 3 7 2 2 2 0 0 9 4 7 1 4 9 8 7 5 9 9 7 1 9 2 4 1 6 5 8 7 1\n",
      " 6 4 3 1 9 8 0 6 0 6 2 2 9 0 2 6 3 6 2 5 8 7 8 4 9 6 9 1 5 1 1 4 7 0 5 6 5\n",
      " 4 0 2 7 8 0 3 3 1 5 7 7 0 3 3 9 8 0 0 5 9 4 7 8 3 9 8 6 3 4 4 9 7 1 3 9 2\n",
      " 5 1 7 5 0 8 8 1 5 6 2 1 0 2 0 1 4 4 2 8 8 6 6 7 0 6 0 8 5 2 8 5 9 3 4 1 0\n",
      " 1 9 4 4 2 9 6 6 4 9 8 4 9 7 6]\n",
      "True labels: [4 3 3 5 3 8 0 7 1 0 1 1 2 5 8 5 5 9 5 5 0 8 0 1 6 4 9 9 3 2 8 4 9 2 1 9 3\n",
      " 0 8 0 3 1 9 8 6 3 7 2 2 2 0 0 8 4 7 1 4 4 5 7 0 9 9 7 7 9 2 4 7 6 5 8 2 1\n",
      " 6 4 3 1 9 8 0 6 0 6 2 2 9 0 2 6 3 6 2 5 8 7 2 4 9 6 9 1 5 1 1 4 7 0 8 6 5\n",
      " 4 0 2 7 3 0 3 3 1 5 7 7 4 3 3 9 8 6 0 5 7 4 7 8 3 9 8 6 3 4 5 9 7 1 3 8 2\n",
      " 8 1 7 5 0 8 8 1 5 6 2 1 0 2 6 1 4 4 2 8 8 6 6 7 6 6 0 8 5 2 8 5 9 3 4 1 0\n",
      " 1 4 4 4 7 9 6 6 4 9 8 4 9 7 6]\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Apply KNN on MNIST\n",
    "knn_model = KNearestNeighbors(k=3, distance_metric=\"cosine\", weights=\"distance\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict(x_test)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"True labels:\", y_test)\n",
    "accuracy = (y_pred == y_test).sum() / y_test.shape[0]\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
