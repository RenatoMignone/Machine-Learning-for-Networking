{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426a8016",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Lab-6 A classifier from scratch<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39139f5",
   "metadata": {},
   "source": [
    "### Objective: Implement, use and evaluate a classifier (without using specific libraries such as sklearn)\n",
    "1. **Logistic regression** is a binary classification method that maps a linear combination of parameters and variables into two possible classes. Here, you will implement the logistic regression from scratch to better understand how an ML algorithm works. Useful link: <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Wiki</a>.\n",
    "2. **Performance evaluation metrics** are needed to evaluate the outcome of prediction with respect to true labels. Here, you will implement confusion matrix, accuracy, precision, recall and F-measure. Useful link: <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">Wiki</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b6bf32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0959af0",
   "metadata": {},
   "source": [
    "### 1. Dataset - TCP logs\n",
    "The dataset contains traffic information generated by an open-source passive network monitoring tool, namely **tstat**. It automates the collection of packet statistics of traffic aggregates, using real-time monitoring features. Being a passive tool, the typical usage scenario is live monitoring of Internet links, in which all transmitted packets are observed. In case of TCP, Tstat identifies a new flow start when it observes a TCP three-way handshake. Similarly, it identifies a TCP flow end either when it sees the TCP connection teardown, or when it doesnâ€™t observe packets for some time (idle time). A flow is defined by a unique link between the sender and receiver, e.g., a tuple of <em>(IP_Protocol_Type, IP_Source_Address, Source_Port, IP_Destination_Address, Destination_Port)</em>. For a specific flow, tstat calculates a number of statistics of all the packets transmitted over this flow, and then generate a log for such flow with multiple attributes (statistics). A log file is arranged as a simple table where each column is associated to specific information and each row reports the flow during a connection. The log information is a summary of the flow properties. For instance, in the TCP log we can find columns like the starting time of a TCP connection, its duration, the number of sent and received packets, the observed Round Trip Time.\n",
    "![](tstat.png)\n",
    "\n",
    "In this lab, since the focus is on the development of logistic regression from scratch, we only consider a portion of the dataset for simplicity. The data can be found in `log_tcp_part.csv`, in which there are multiple columns, the last one is the class label, indicating the flow is from either **google** or **youtube**, and the rest are features. Your job is a binary classification task to classify the domain of each flow (row) **from scratch**, including:\n",
    "- Build a logistic regression model,\n",
    "- Evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1d837",
   "metadata": {},
   "source": [
    "1. Load the dataset.\n",
    "2. Get the list of features (columns 1 to 10).\n",
    "3. Add a new column and assign numerical class labels of -1 and 1 to google and youtube.\n",
    "4. Answering the following questions:\n",
    "    - How many features do we have? 10\n",
    "    - How many samples do we have in total? 20000\n",
    "    - How many samples do we have for each class? Are they similar? 10000 and 10000, they are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70294ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google     10000\n",
       "youtube    10000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"log_tcp_part.csv\")\n",
    "features = dataframe.columns\n",
    "\n",
    "dataframe[\"m_classes\"] = np.where(dataframe[\"class\"]=='google', -1 ,1)\n",
    "dataframe\n",
    "\n",
    "dataframe[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8cc80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Implement your logistic regression learning algorithm\n",
    "Here you will need to construct a class in which you need to define two functions besides the class initialization:\n",
    "- `fit`. In this method you will perform ERM. Learn the parameters of the model (i.e., the hypothesis h) from training with gradient descent\n",
    "- `predict`. In this method given one  sample x (or more) you will perform the inference $sign(h(x))$ to obtain class labels.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- The linear function used in the logistic regression is the following: $h(x)=w^T x +b $, where b is a scalar bias.\n",
    "- Logistic loss: $L((x,y),h)=\\log(1+e^{-y h(x)})$\n",
    "- ERM: $\\min_{w,b} f(w,b)=\\frac{1}{m}\\sum_{i=1}^{m} \\log(1+e^{-y^{(i)} h(x^{(i)})})$\n",
    "- Gradient for weight: $\\nabla_w f(w,b) = \\frac{1}{m} \\sum_i \\frac{-y^{(i)}x^{(i)}}{(1+e^{-y^{(i)}h(x^{(i)})})}$\n",
    "- Gradient for bias: $\\nabla_b f(w,b)= \\frac{1}{m} \\sum_i \\frac{-y^{(i)}}{(1+e^{-y^{(i)}h(x^{(i)})})}$\n",
    "- Update the parameters: $w \\leftarrow w - \\alpha \\nabla w$, $b \\leftarrow b - \\alpha  \\nabla b$\n",
    "\n",
    "Notice that the sigmoid function $f(z) = \\frac{1}{1 + e^{-z}}$ appears multiple times. You can write also a method for the sigmoid function to help you in the computation. By considering f(z), the gradients rewrite as:\n",
    "\n",
    "- Gradient for weight: $\\nabla_w f(w,b) = \\frac{1}{m} \\sum_i ({f(h(x^{(i)})) - y^{(i)}})x^{(i)}$\n",
    "- Gradient for bias: $\\nabla_b f(w,b) = \\frac{1}{m} \\sum_i ({f(h(x^{(i)})) - y^{(i)}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8428481f-b253-4daf-952a-31304983cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.iloc[:,0:10]\n",
    "y = dataframe.iloc[:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "90a02f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate, num_iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.b = None\n",
    "        self.W = None\n",
    "        self.ER = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.b = 0\n",
    "        self.W = np.random.uniform(-1,1,10)\n",
    "        self.ER = 0\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            z = X.dot(self.W) + self.b\n",
    "            \n",
    "            sigmoide = self.sigmoid(z)\n",
    "            \n",
    "            loss = np.log(1 + np.exp(-y * z))\n",
    "            ER = (1/len(y))*np.sum(loss)\n",
    "            \n",
    "            if(ER < self.ER):\n",
    "                self.ER = ER\n",
    "                dw = (1/len(y))*np.dot(X.T, (sigmoide-y))\n",
    "                db = (1/len(y))*np.sum(sigmoide-y)\n",
    "                self.W -= dw*self.learning_rate\n",
    "                self.b -= db*self.learning_rate\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        lista_labels = []\n",
    "        \n",
    "        for index,element in X.iterrows():\n",
    "            z = element.dot(self.W) + self.b\n",
    "            sigmoide = self.sigmoid(z)\n",
    "            \n",
    "            if sigmoide > 0.5:\n",
    "                lista_labels = lista_labels + [1]\n",
    "            else:\n",
    "                lista_labels = lista_labels + [-1]\n",
    "                \n",
    "        return lista_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc478b78",
   "metadata": {},
   "source": [
    "### 3. Use the model\n",
    "- Initialize your model with predefined learning rate of `0.1` and iterations of `100`.\n",
    "- Fit your model with features and targets.\n",
    "- Get the prediction with features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af5a590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(0.1,100)\n",
    "\n",
    "classifier.fit(X,y)\n",
    "\n",
    "x_test = X[9900:10100]\n",
    "y_true = y[9900:10100]\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ad9e7",
   "metadata": {},
   "source": [
    "### 4. Model evaluation\n",
    "With predicted class labels and ground truths, we now evaluate the model performance through confusion matrix and numerical metrics. Specifically, you need to derive the following:\n",
    "- Confusion matrix - Note that, you should indicate the corresponding quantity of each element in the table. Here positive is class 1 and negative is class -1:\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & \\textbf{Predicted Positive} & \\textbf{Predicted Negative} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Positive} & \\text{True Positive (TP)} & \\text{False Negative (FN)} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual Negative} & \\text{False Positive (FP)} & \\text{True Negative (TN)} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "- Precision of each class and the average value:\n",
    "$\\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP) + False Positive (FP)}}$\n",
    "- Recall of each class and the average value:\n",
    "$\\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP) + False Negative (FN)}}$\n",
    "- F1-score of each class and the average value:\n",
    "$F_1 = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "- Accuracy:\n",
    "$\\frac{\\text{True Positive (TP) + True Negative (TN)}}{\\text{True Positive (TP) + True Negative (TN) + False Positive (FP) + False Negative (FN)}}$\n",
    "- Answering the following questions:\n",
    "    - Do you have same performance between classes? If not, which one performs better?\n",
    "    - Change the parameters of learning rate or number of iterations. Do you have same performance? Better or Worse? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "15b74982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "TP: 63, FP: 9, FN: 37, TN: 91\n",
      "\n",
      "Metrics:\n",
      "Precision (Positive Class): 0.875\n",
      "Precision (Negative Class): 0.7109375\n",
      "Average Precision: 0.79296875\n",
      "Recall (Positive Class): 0.63\n",
      "Recall (Negative Class): 0.91\n",
      "Average Recall: 0.77\n",
      "F1-Score (Positive Class): 0.7325581395348838\n",
      "F1-Score (Negative Class): 0.7982456140350876\n",
      "Average F1-Score: 0.7654018767849857\n",
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Initialize confusion matrix elements\n",
    "tp = fp = fn = tn = 0\n",
    "\n",
    "# Compute TP, FP, FN, and TN\n",
    "for actual, predicted in zip(y_true, y_pred):\n",
    "    if actual == 1 and predicted == 1:\n",
    "        tp += 1  # True Positive\n",
    "    elif actual == -1 and predicted == 1:\n",
    "        fp += 1  # False Positive\n",
    "    elif actual == 1 and predicted == -1:\n",
    "        fn += 1  # False Negative\n",
    "    elif actual == -1 and predicted == -1:\n",
    "        tn += 1  # True Negative\n",
    "\n",
    "# Precision, Recall, F1-Score, and Accuracy calculations\n",
    "precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "precision_neg = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "recall_neg = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "# Average precision and recall\n",
    "precision_avg = (precision_pos + precision_neg) / 2\n",
    "recall_avg = (recall_pos + recall_neg) / 2\n",
    "\n",
    "# F1-Scores\n",
    "f1_pos = (2 * precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) > 0 else 0\n",
    "f1_neg = (2 * precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) > 0 else 0\n",
    "f1_avg = (f1_pos + f1_neg) / 2\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Precision (Positive Class): {precision_pos}\")\n",
    "print(f\"Precision (Negative Class): {precision_neg}\")\n",
    "print(f\"Average Precision: {precision_avg}\")\n",
    "print(f\"Recall (Positive Class): {recall_pos}\")\n",
    "print(f\"Recall (Negative Class): {recall_neg}\")\n",
    "print(f\"Average Recall: {recall_avg}\")\n",
    "print(f\"F1-Score (Positive Class): {f1_pos}\")\n",
    "print(f\"F1-Score (Negative Class): {f1_neg}\")\n",
    "print(f\"Average F1-Score: {f1_avg}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
